// This file is auto-generated by @hey-api/openapi-ts

export type AudioGenerationRequest = {
  text: string;
  language?: string | null;
  speaker?: string | null;
  sdp_ratio?: number;
  noise_scale?: number;
  noise_scale_w?: number;
  speed?: number;
  /**
   * The name of the model to query.
   */
  model?: string;
};

export type AudioGenerationResponse = {
  /**
   * The generated audio
   */
  audio: string | null;
  inference_time: number;
  sample_rate: number;
  usage: number;
};

export type ChatCompletionContentPartImageParam = {
  image_url: ImageUrl;
  type: "image_url";
};

export type ChatCompletionContentPartInputAudioParam = {
  input_audio: InputAudio;
  type: "input_audio";
};

export type ChatCompletionContentPartTextParam = {
  text: string;
  type: "text";
};

export type ChatCompletionLogProb = {
  token: string;
  logprob?: number;
  bytes?: Array<number> | null;
};

export type ChatCompletionLogProbs = {
  content?: Array<ChatCompletionLogProbsContent> | null;
};

export type ChatCompletionLogProbsContent = {
  token: string;
  logprob?: number;
  bytes?: Array<number> | null;
  top_logprobs?: Array<ChatCompletionLogProb>;
};

export type ChatCompletionNamedFunction = {
  name: string;
};

export type ChatCompletionNamedToolChoiceParam = {
  function: ChatCompletionNamedFunction;
  type?: "function";
};

export type ChatCompletionRequest = {
  /**
   * A list of messages comprising the conversation.
   */
  messages: Array<ChatMessage>;
  /**
   * The name of the model to query.
   */
  model?: string;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim..
   */
  frequency_penalty?: number | null;
  logit_bias?: {
    [key: string]: number;
  } | null;
  /**
   * Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.
   */
  logprobs?: boolean | null;
  /**
   * An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.
   */
  top_logprobs?: number | null;
  /**
   * The maximum number of tokens to generate.
   */
  max_tokens?: number | null;
  /**
   * How many completions to generate for each prompt.
   */
  n?: number | null;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
   */
  presence_penalty?: number | null;
  seed?: number | null;
  /**
   * A list of string sequences that will truncate (stop) inference text output. For example,  will stop generation as soon as the model generates the given token.
   */
  stop?: string | Array<string> | null;
  /**
   * Whether to stream the response back to the client. If true, stream tokens as Server-Sent Events as the model generates them instead of waiting for the full model response. If false, return a single JSON object containing the results.
   */
  stream?: boolean | null;
  /**
   * A decimal number that determines the degree of randomness in the response. A value of 1 will always yield the same output. A temperature less than 1 favors more correctness and is appropriate for question answering or summarization. A value greater than 1 introduces more randomness in the output.
   */
  temperature?: number | null;
  /**
   * The top_p (nucleus) parameter is used to dynamically adjust the number of choices for each predicted token based on the cumulative probabilities. It specifies a probability threshold, below which all less likely tokens are filtered out. This technique helps to maintain diversity and generate more fluent and natural-sounding text.
   */
  top_p?: number | null;
  tools?: Array<ChatCompletionToolsParam> | null;
  tool_choice?: "none" | "auto" | ChatCompletionNamedToolChoiceParam | null;
  /**
   * A unique identifier representing end-user.
   */
  user?: string | null;
  top_k?: number | null;
  min_p?: number | null;
  repetition_penalty?: number | null;
};

export type ChatCompletionResponse = {
  id?: string;
  object?: string;
  created?: number;
  model: string;
  choices: Array<ChatCompletionResponseChoice>;
  usage: UsageInfo;
};

export type ChatCompletionResponseChoice = {
  index: number;
  message: ChatResponseMessage;
  finish_reason?: ("stop" | "length" | "tool_calls" | "content_filter" | "function_call") | null;
  logprobs?: ChatCompletionLogProbs | null;
};

export type ChatCompletionToolsParam = {
  type?: "function";
  function: FunctionDefinition;
};

export type ChatMessage = {
  /**
   * The role of the messages author. Choice between: system, user, or assistant.
   */
  role?: string;
  /**
   * The contents of the message.
   */
  content?:
    | string
    | Array<
        | ChatCompletionContentPartTextParam
        | ChatCompletionContentPartImageParam
        | ChatCompletionContentPartInputAudioParam
        | File
      >;
  tool_calls?: Array<ToolCall>;
};

export type ChatResponseMessage = {
  role: string;
  content?: string | null;
  tool_calls?: Array<ToolCall>;
};

export type CompletionLogProbs = {
  text_offset?: Array<number>;
  token_logprobs?: Array<number | null>;
  tokens?: Array<string>;
  top_logprobs?: Array<{
    [key: string]: number;
  } | null>;
};

export type CompletionRequest = {
  /**
   * The name of the model to query.
   */
  model?: string;
  /**
   * The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.
   */
  prompt: Array<number> | Array<Array<number>> | string | Array<string>;
  images?: Array<ChatCompletionContentPartImageParam> | null;
  /**
   * Generates best_of completions server-side and returns the `best` (the one with the highest log probability per token). Results cannot be streamed. When used with n, best_of controls the number of candidate completions and n specifies how many to return - best_of must be greater than n.
   */
  best_of?: number | null;
  /**
   * Echo back the prompt in addition to the completion
   */
  echo?: boolean | null;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim..
   */
  frequency_penalty?: number | null;
  logit_bias?: {
    [key: string]: number;
  } | null;
  /**
   * Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.
   */
  logprobs?: number | null;
  /**
   * The maximum number of tokens to generate.
   */
  max_tokens?: number | null;
  /**
   * How many completions to generate for each prompt.
   */
  n?: number | null;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
   */
  presence_penalty?: number | null;
  seed?: number | null;
  /**
   * A list of string sequences that will truncate (stop) inference text output. For example,  will stop generation as soon as the model generates the given token.
   */
  stop?: string | Array<string> | null;
  /**
   * Whether to stream the response back to the client. If true, stream tokens as Server-Sent Events as the model generates them instead of waiting for the full model response. If false, return a single JSON object containing the results.
   */
  stream?: boolean | null;
  /**
   * A decimal number that determines the degree of randomness in the response. A value of 1 will always yield the same output. A temperature less than 1 favors more correctness and is appropriate for question answering or summarization. A value greater than 1 introduces more randomness in the output.
   */
  temperature?: number | null;
  /**
   * The top_p (nucleus) parameter is used to dynamically adjust the number of choices for each predicted token based on the cumulative probabilities. It specifies a probability threshold, below which all less likely tokens are filtered out. This technique helps to maintain diversity and generate more fluent and natural-sounding text.
   */
  top_p?: number | null;
  /**
   * A unique identifier representing end-user.
   */
  user?: string | null;
  top_k?: number | null;
  min_p?: number | null;
  repetition_penalty?: number | null;
  raw_mode?: boolean | null;
};

export type CompletionResponse = {
  id?: string;
  choices: Array<CompletionResponseChoice>;
  created?: number;
  model: string;
  system_fingerprint?: string;
  object?: string;
  usage: UsageInfo;
};

export type CompletionResponseChoice = {
  finish_reason?: ("stop" | "length" | "content_filter") | null;
  index: number;
  logprobs?: CompletionLogProbs | null;
  text: string;
};

export type File = {
  file: FileFile;
  type: "file";
};

export type FileFile = {
  file_data?: string;
  file_id?: string;
  file_name?: string;
};

export type FunctionCall = {
  name: string;
  arguments: string;
};

export type FunctionDefinition = {
  name: string;
  description?: string | null;
  parameters?: {
    [key: string]: unknown;
  } | null;
};

export type HttpValidationError = {
  detail?: Array<ValidationError>;
};

export type ImageGenerationRequest = {
  model_name: string;
  height: number;
  width: number;
  prompt: string;
  prompt_2?: string | null;
  negative_prompt?: string | null;
  negative_prompt_2?: string | null;
  /**
   * backend=auto will automatically select the backend from ['torch', 'tvm']. The torch backend supports various resolutions, but is slower. The tvm backend is faster, but only supports specific heights and widths.
   */
  backend?: string;
  /**
   * Random noise seed, default a random seed
   */
  seed?: number | null;
  /**
   * Higher guidance scale encourages to generate images that are closely linked to the text `prompt`
   */
  cfg_scale?: number;
  sampler?: string | null;
  steps?: number;
  style_preset?: string | null;
  enable_refiner?: boolean;
  /**
   * If the controlnet_name is provided, please also send the image as the guidance of generation
   */
  controlnet_name?: string | null;
  /**
   * The ControlNet input condition to provide guidance to the generation
   */
  controlnet_image?: string | null;
  image?: string | null;
  lora?: {
    [key: string]: number;
  };
  strength?: number | null;
};

export type ImageGenerationResponse = {
  images?: Array<ImageOutput>;
  inference_time: number;
};

export type ImageOutput = {
  index: number;
  /**
   * The generated image
   */
  image?: string | null;
  /**
   * Random seed used to generate the image.
   */
  random_seed: number;
};

export type ImageUrl = {
  url: string;
  detail?: "auto" | "low" | "high";
};

export type InputAudio = {
  data: string;
  format: "wav" | "mp3";
};

export type JanusOutput = {
  text: string;
  images?: Array<string>;
  error_code: number;
  error_message: string;
};

export type JanusRequest = {
  /**
   * A list of messages comprising the conversation.
   */
  messages: Array<ChatMessage>;
  max_tokens?: number;
  temperature?: number;
  n?: number;
  cfg_weight?: number;
  image_size?: number;
  type?: string;
};

export type ListResponse = {
  object?: string;
  data: Array<unknown>;
};

export type Meme = "Monad" | "Wifhat";

export type MemeGenerationRequest = {
  meme_name?: Meme;
  prompt: string;
  /**
   * Random noise seed, default a random seed
   */
  seed?: number | null;
  /**
   * Higher guidance scale encourages to generate images that are closely linked to the text `prompt`
   */
  cfg_scale?: number;
  steps?: number;
};

export type Model = {
  name: string;
  type: ModelType;
  price: number;
  description: string;
  id: number;
};

export type ModelType = "llm" | "image" | "tts";

export type Role = "user" | "admin" | "pro" | "elite";

export type ToolCall = {
  id?: string;
  type?: "function";
  function: FunctionCall;
};

export type Provider = "google.com" | "github.com" | "password";

export type User = {
  email: string;
  picture?: string | null;
  provider?: Provider | null;
  email_verified?: boolean | null;
  name?: string | null;
  public_key?: string | null;
  onboarded_at?: string | null;
  onboarded_for?: string | null;
  meta?: {
    [key: string]: unknown;
  } | null;
  id: string;
  is_active: boolean;
  api_key: string;
  role: string;
};

export type UsageInfo = {
  prompt_tokens?: number;
  total_tokens?: number;
  completion_tokens?: number | null;
};

export type ValidationError = {
  loc: Array<string | number>;
  msg: string;
  type: string;
};

export type CreateChatCompletionV1ChatCompletionsPostData = {
  body: ChatCompletionRequest;
  path?: never;
  query?: never;
  url: "/v1/chat/completions";
};

export type CreateChatCompletionV1ChatCompletionsPostErrors = {
  /**
   * Validation Error
   */
  422: HttpValidationError;
};

export type CreateChatCompletionV1ChatCompletionsPostError =
  CreateChatCompletionV1ChatCompletionsPostErrors[keyof CreateChatCompletionV1ChatCompletionsPostErrors];

export type CreateChatCompletionV1ChatCompletionsPostResponses = {
  /**
   * Successful Response
   */
  200: ChatCompletionResponse;
};

export type CreateChatCompletionV1ChatCompletionsPostResponse =
  CreateChatCompletionV1ChatCompletionsPostResponses[keyof CreateChatCompletionV1ChatCompletionsPostResponses];

export type CreateCompletionV1CompletionsPostData = {
  body: CompletionRequest;
  path?: never;
  query?: never;
  url: "/v1/completions";
};

export type CreateCompletionV1CompletionsPostErrors = {
  /**
   * Validation Error
   */
  422: HttpValidationError;
};

export type CreateCompletionV1CompletionsPostError =
  CreateCompletionV1CompletionsPostErrors[keyof CreateCompletionV1CompletionsPostErrors];

export type CreateCompletionV1CompletionsPostResponses = {
  /**
   * Successful Response
   */
  200: CompletionResponse;
};

export type CreateCompletionV1CompletionsPostResponse =
  CreateCompletionV1CompletionsPostResponses[keyof CreateCompletionV1CompletionsPostResponses];

export type UpdateConfigsV1ImageUpdateConfigsPostData = {
  body?: never;
  path?: never;
  query?: never;
  url: "/v1/image/update_configs";
};

export type UpdateConfigsV1ImageUpdateConfigsPostResponses = {
  /**
   * Successful Response
   */
  200: unknown;
};

export type CreateImageGenerationV1ImagesGenerationsPostData = {
  body: ImageGenerationRequest;
  path?: never;
  query?: never;
  url: "/v1/images/generations";
};

export type CreateImageGenerationV1ImagesGenerationsPostErrors = {
  /**
   * Validation Error
   */
  422: HttpValidationError;
};

export type CreateImageGenerationV1ImagesGenerationsPostError =
  CreateImageGenerationV1ImagesGenerationsPostErrors[keyof CreateImageGenerationV1ImagesGenerationsPostErrors];

export type CreateImageGenerationV1ImagesGenerationsPostResponses = {
  /**
   * Successful Response
   */
  200: ImageGenerationResponse;
};

export type CreateImageGenerationV1ImagesGenerationsPostResponse =
  CreateImageGenerationV1ImagesGenerationsPostResponses[keyof CreateImageGenerationV1ImagesGenerationsPostResponses];

export type CreateImageGenerationV1ImageGenerationPostData = {
  body: ImageGenerationRequest;
  path?: never;
  query?: never;
  url: "/v1/image/generation";
};

export type CreateImageGenerationV1ImageGenerationPostErrors = {
  /**
   * Validation Error
   */
  422: HttpValidationError;
};

export type CreateImageGenerationV1ImageGenerationPostError =
  CreateImageGenerationV1ImageGenerationPostErrors[keyof CreateImageGenerationV1ImageGenerationPostErrors];

export type CreateImageGenerationV1ImageGenerationPostResponses = {
  /**
   * Successful Response
   */
  200: ImageGenerationResponse;
};

export type CreateImageGenerationV1ImageGenerationPostResponse =
  CreateImageGenerationV1ImageGenerationPostResponses[keyof CreateImageGenerationV1ImageGenerationPostResponses];

export type CreateMonadMemeGenerationV1ImageMemeGenerationPostData = {
  body: MemeGenerationRequest;
  path?: never;
  query?: never;
  url: "/v1/image/meme_generation";
};

export type CreateMonadMemeGenerationV1ImageMemeGenerationPostErrors = {
  /**
   * Validation Error
   */
  422: HttpValidationError;
};

export type CreateMonadMemeGenerationV1ImageMemeGenerationPostError =
  CreateMonadMemeGenerationV1ImageMemeGenerationPostErrors[keyof CreateMonadMemeGenerationV1ImageMemeGenerationPostErrors];

export type CreateMonadMemeGenerationV1ImageMemeGenerationPostResponses = {
  /**
   * Successful Response
   */
  200: ImageGenerationResponse;
};

export type CreateMonadMemeGenerationV1ImageMemeGenerationPostResponse =
  CreateMonadMemeGenerationV1ImageMemeGenerationPostResponses[keyof CreateMonadMemeGenerationV1ImageMemeGenerationPostResponses];

export type ModelsPriceModelsModelNamePriceGetData = {
  body?: never;
  path: {
    model_name: string;
  };
  query?: never;
  url: "/models/{model_name}/price";
};

export type ModelsPriceModelsModelNamePriceGetErrors = {
  /**
   * Validation Error
   */
  422: HttpValidationError;
};

export type ModelsPriceModelsModelNamePriceGetError =
  ModelsPriceModelsModelNamePriceGetErrors[keyof ModelsPriceModelsModelNamePriceGetErrors];

export type ModelsPriceModelsModelNamePriceGetResponses = {
  /**
   * Successful Response
   */
  200: unknown;
};

export type CreateImageGenerationV1AudioGenerationPostData = {
  body: AudioGenerationRequest;
  path?: never;
  query?: never;
  url: "/v1/audio/generation";
};

export type CreateImageGenerationV1AudioGenerationPostErrors = {
  /**
   * Validation Error
   */
  422: HttpValidationError;
};

export type CreateImageGenerationV1AudioGenerationPostError =
  CreateImageGenerationV1AudioGenerationPostErrors[keyof CreateImageGenerationV1AudioGenerationPostErrors];

export type CreateImageGenerationV1AudioGenerationPostResponses = {
  /**
   * Successful Response
   */
  200: AudioGenerationResponse;
};

export type CreateImageGenerationV1AudioGenerationPostResponse =
  CreateImageGenerationV1AudioGenerationPostResponses[keyof CreateImageGenerationV1AudioGenerationPostResponses];

export type CreateJanusGenerationV1JanusGenerationPostData = {
  body: JanusRequest;
  path?: never;
  query?: never;
  url: "/v1/janus/generation";
};

export type CreateJanusGenerationV1JanusGenerationPostErrors = {
  /**
   * Validation Error
   */
  422: HttpValidationError;
};

export type CreateJanusGenerationV1JanusGenerationPostError =
  CreateJanusGenerationV1JanusGenerationPostErrors[keyof CreateJanusGenerationV1JanusGenerationPostErrors];

export type CreateJanusGenerationV1JanusGenerationPostResponses = {
  /**
   * Successful Response
   */
  200: JanusOutput;
};

export type CreateJanusGenerationV1JanusGenerationPostResponse =
  CreateJanusGenerationV1JanusGenerationPostResponses[keyof CreateJanusGenerationV1JanusGenerationPostResponses];

export type ShowModelsV1ModelsGetData = {
  body?: never;
  path?: never;
  query?: never;
  url: "/v1/models";
};

export type ShowModelsV1ModelsGetResponses = {
  /**
   * Successful Response
   */
  200: ListResponse;
};

export type ShowModelsV1ModelsGetResponse =
  ShowModelsV1ModelsGetResponses[keyof ShowModelsV1ModelsGetResponses];

export type ClientOptions = {
  baseUrl: `${string}://openapi-spec.json` | (string & {});
};
